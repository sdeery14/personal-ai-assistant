openapi: 3.1.0
info:
  title: Personal AI Assistant - Chat API
  description: |
    Streaming chat endpoint for interacting with LLM assistant.

    **Feature**: Core Streaming Chat API (001)
    **Protocol**: Server-Sent Events (SSE) over HTTP
    **Authentication**: None (single-user development mode)
  version: 0.1.0
  contact:
    name: Feature 001 - Streaming Chat API
    url: https://github.com/yourorg/personal-ai-assistant

servers:
  - url: http://localhost:8000
    description: Local development server

paths:
  /chat:
    post:
      summary: Send a message and receive streamed response
      description: |
        Accepts a text message and streams back the LLM's response in real-time
        using Server-Sent Events (SSE). Each chunk includes content, sequence number,
        and final flag.

        **Streaming Format**: SSE with `data:` prefix per chunk
        **Timeout**: 30 seconds
        **Correlation ID**: Generated automatically, included in all chunks
      operationId: chat
      tags:
        - Chat
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/ChatRequest'
            examples:
              simple_question:
                summary: Simple question
                value:
                  message: "What is the capital of France?"
              with_options:
                summary: With model and token options
                value:
                  message: "Explain quantum computing"
                  model: "gpt-3.5-turbo"
                  max_tokens: 1000
      responses:
        '200':
          description: Successful streaming response
          headers:
            X-Correlation-Id:
              description: Unique request tracking identifier
              schema:
                type: string
                format: uuid
          content:
            text/event-stream:
              schema:
                $ref: '#/components/schemas/StreamChunk'
              examples:
                streaming_chunks:
                  summary: Example SSE stream
                  value: |
                    data: {"content":"The","sequence":0,"is_final":false,"correlation_id":"550e8400-e29b-41d4-a716-446655440000"}

                    data: {"content":" capital","sequence":1,"is_final":false,"correlation_id":"550e8400-e29b-41d4-a716-446655440000"}

                    data: {"content":" of France is Paris.","sequence":2,"is_final":true,"correlation_id":"550e8400-e29b-41d4-a716-446655440000"}
        '400':
          description: Invalid request (validation error)
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ErrorResponse'
              examples:
                empty_message:
                  summary: Empty message error
                  value:
                    error: "Validation error"
                    detail: "Message cannot be empty or whitespace only"
                    correlation_id: "550e8400-e29b-41d4-a716-446655440000"
                invalid_model:
                  summary: Invalid model selection
                  value:
                    error: "Validation error"
                    detail: "Model must be one of ['gpt-4', 'gpt-3.5-turbo']"
                    correlation_id: "550e8400-e29b-41d4-a716-446655440000"
        '500':
          description: Internal server error (OpenAI API failure, etc.)
          content:
            text/event-stream:
              schema:
                $ref: '#/components/schemas/StreamChunk'
              examples:
                api_error:
                  summary: OpenAI API error streamed to client
                  value: |
                    data: {"content":"[ERROR] The AI service is temporarily unavailable. This may be due to high demand or a service outage. Please try again in a few moments.","sequence":0,"is_final":true,"correlation_id":"550e8400-e29b-41d4-a716-446655440000"}
        '504':
          description: Gateway timeout (request exceeded 30 second limit)
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ErrorResponse'
              examples:
                timeout:
                  summary: Request timeout
                  value:
                    error: "Request timeout"
                    detail: "The request took longer than 30 seconds to complete. The AI service may be experiencing delays. Please try a shorter message or try again later."
                    correlation_id: "550e8400-e29b-41d4-a716-446655440000"

  /health:
    get:
      summary: Health check endpoint
      description: Returns service health status
      operationId: health_check
      tags:
        - Monitoring
      responses:
        '200':
          description: Service is healthy
          content:
            application/json:
              schema:
                type: object
                properties:
                  status:
                    type: string
                    example: "healthy"
                  timestamp:
                    type: string
                    format: date-time
                    example: "2026-01-28T14:32:10.123456Z"

components:
  schemas:
    ChatRequest:
      type: object
      required:
        - message
      properties:
        message:
          type: string
          minLength: 1
          maxLength: 8000
          description: User's text input to the assistant
          example: "What is the capital of France?"
        model:
          type: string
          enum:
            - gpt-4
            - gpt-3.5-turbo
          default: gpt-4
          description: OpenAI model to use for completion
          example: "gpt-4"
        max_tokens:
          type: integer
          minimum: 1
          maximum: 4000
          default: 2000
          description: Maximum tokens in the response
          example: 2000

    StreamChunk:
      type: object
      required:
        - content
        - sequence
        - is_final
        - correlation_id
      properties:
        content:
          type: string
          description: Text fragment from the LLM response
          example: "The capital"
        sequence:
          type: integer
          minimum: 0
          description: Zero-indexed chunk number in the stream
          example: 0
        is_final:
          type: boolean
          description: True if this is the last chunk in the stream
          example: false
        correlation_id:
          type: string
          format: uuid
          description: Unique identifier for this request
          example: "550e8400-e29b-41d4-a716-446655440000"

    ErrorResponse:
      type: object
      required:
        - error
        - detail
        - correlation_id
      properties:
        error:
          type: string
          description: Error category
          example: "Validation error"
        detail:
          type: string
          description: |
            Human-readable error message following constitutional UX principle:
            - What happened
            - Why it happened
            - What the user can do
          example: "Message cannot be empty or whitespace only"
        correlation_id:
          type: string
          format: uuid
          description: Request tracking identifier for debugging
          example: "550e8400-e29b-41d4-a716-446655440000"

tags:
  - name: Chat
    description: Core chat interaction endpoint
  - name: Monitoring
    description: Service health and monitoring
